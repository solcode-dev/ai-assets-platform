import base64
import logging
from typing import Optional
from celery import Celery
from app.core.config import settings
from app.core.exceptions import ExternalServiceException, ValidationException

logger = logging.getLogger(__name__)

celery_app = Celery(
    "worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

# Pillow import for dimension extraction
import io
from PIL import Image

celery_app.conf.task_routes = {
    "generate_asset_task": {"queue": "main_queue"},
    "process_asset_metadata": {"queue": "analysis_queue"},
    "app.worker.tasks.*": {"queue": "main_queue"}
}

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)

# -------------------------------------------------------------------------
# Worker Process Initialization Check
# -------------------------------------------------------------------------
from celery.signals import worker_process_init
from app.infrastructure.database import worker_engine

@worker_process_init.connect
def init_worker(**kwargs):
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ í¬í¬ëœ ì§í›„ í˜¸ì¶œë©ë‹ˆë‹¤.
    ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ë¡œë¶€í„° ìƒì†ë°›ì€ ì»¤ë„¥ì…˜ í’€ì„ íê¸°í•˜ê³ ,
    ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìì²´ì ì¸ ì»¤ë„¥ì…˜ì„ ë§ºë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
    """
    try:
        worker_engine.sync_engine.dispose()
        # âœ¨ Redis ì»¤ë„¥ì…˜ë„ ì´ˆê¸°í™”í•˜ì—¬ ìì‹ í”„ë¡œì„¸ìŠ¤ ê°„ ì¶©ëŒ ë°©ì§€
        from app.core.events import event_bus
        import asyncio
        try:
            # ë™ê¸° í™˜ê²½ì´ë¯€ë¡œ ìƒˆë¡œìš´ ë£¨í”„ë¡œ ë‹«ê¸° ì‹œë„
            loop = asyncio.new_event_loop()
            loop.run_until_complete(event_bus.close())
            loop.close()
        except:
            pass
        logger.info("âœ… Worker engine and EventBus reset completed.")
    except Exception as e:
        logger.error(f"âŒ Failed to reset worker resources: {e}")



@celery_app.task(name="generate_asset_task", bind=True, max_retries=settings.MAX_TASK_RETRIES)
def generate_asset_task(
    self,
    job_id: str, 
    prompt: str, 
    mode: str,
    source_image: Optional[str] = None,
    source_image_mime_type: Optional[str] = None
):
    from app.infrastructure.database import worker_session_maker
    from app.infrastructure.repositories import SQLAlchemyAssetRepository
    from app.infrastructure.storage import get_storage_provider
    from app.infrastructure.vertex_ai import get_ai_client
    from app.infrastructure.models import JobStatus

    # ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ìš© í—¬í¼ (ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ í•­ìƒ ìƒˆ ì„¸ì…˜ ì‚¬ìš©)
    async def _safe_update_status(status_val: str, path: Optional[str] = None, width: Optional[int] = None, height: Optional[int] = None, error_msg: Optional[str] = None):
        logger.debug(f"[HAWKEYE:WORKER] _safe_update_status called: job_id={job_id}, status={status_val}, error={error_msg[:30] if error_msg else None}")
        try:
            # NullPool ì—”ì§„ì„ ì‚¬ìš©í•˜ëŠ” ì›Œì»¤ ì „ìš© ì„¸ì…˜ ìƒì„±
            async with worker_session_maker() as temp_session:
                async with temp_session.begin():
                    repo = SQLAlchemyAssetRepository(temp_session)
                    await repo.update_status(job_id, status_val, file_path=path, width=width, height=height, error_msg=error_msg)
            
            # ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŒ… (Redis Pub/Sub)
            from app.core.events import event_bus
            from datetime import datetime
            event_data = {
                "job_id": job_id,
                "status": status_val,
                "result_url": path,
                "error": error_msg,
                "updated_at": datetime.utcnow().isoformat() + "Z"
            }
            # Redis ë°œí–‰ ì‹œ íƒ€ì„ì•„ì›ƒ/ì˜ˆì™¸ì— ë” ë¯¼ê°í•˜ê²Œ ëŒ€ì‘
            import asyncio
            try:
                await asyncio.wait_for(event_bus.publish("asset_updates", event_data), timeout=1.5)
                logger.debug(f"[HAWKEYE:WORKER] Broadcast success: {status_val} for {job_id}")
            except Exception as eb_err:
                logger.warning(f"[HAWKEYE:WORKER] Broadcast delayed/failed (non-critical): {eb_err}")

        except Exception as e:
            logger.error(f"[HAWKEYE:WORKER] CRITICAL: Status update FAILED for {job_id}: {e}")


    async def _process():
        logger.info(f"ğŸš€ Worker started processing job: job_id={job_id}, mode={mode}")
        
        try:
            # 1. ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ë³€ê²½ (ì¬ì‹œë„ ì¤‘ì´ë©´ ì´ë¯¸ PROCESSINGì´ë¯€ë¡œ ì—…ë°ì´íŠ¸ ìƒëµ)
            retry_count = self.request.retries
            if retry_count == 0:
                await _safe_update_status(JobStatus.PROCESSING.value, error_msg="")
            else:
                logger.info(f"[HAWKEYE:WORKER] Task resumed (Retry {retry_count}). Skipping redundant PROCESSING update to preserve message.")

            
            # 2. AI ëª¨ë¸ í˜¸ì¶œ
            logger.info(f"Calling Vertex AI client for job_id={job_id}, mode={mode}")
            
            storage = get_storage_provider()
            ai_client = get_ai_client()
            
            if mode == "text-to-image":
                content = await ai_client.generate_image(prompt)
                extension = "png"
            elif mode == "text-to-video":
                content = await ai_client.generate_video_from_text(prompt)
                extension = "mp4"
            elif mode == "image-to-video":
                if not source_image:
                    raise ValidationException("Image-to-Video mode requires source_image")
                try:
                    image_bytes = base64.b64decode(source_image)
                except Exception as e:
                    raise ValidationException(f"Invalid base64 image: {e}") from e
                
                content = await ai_client.generate_video_from_image(prompt, image_bytes, source_image_mime_type)
                extension = "mp4"
            else:
                raise ValidationException(f"Unknown generation mode: {mode}")
            
            logger.info(f"AI content generated for job_id={job_id}, size={len(content)} bytes")

            # 2.5 ì´ë¯¸ì§€ í¬ê¸° ì¸¡ì • (Lazy Loading)
            width = None
            height = None
            if mode == "text-to-image" and content:
                try:
                    with Image.open(io.BytesIO(content)) as img:
                        width, height = img.size
                        logger.info(f"Image dimensions extracted: {width}x{height}")
                except Exception as e:
                    logger.warning(f"Failed to extract image dimensions: {e}")

            # 3. íŒŒì¼ ì €ì¥
            filename = f"{job_id}.{extension}"
            logger.info(f"Saving file for job_id={job_id}: {filename}")
            file_path = await storage.save_file(content, filename)
            logger.info(f"File saved at: {file_path}")
            
            # 4. DB ì—…ë°ì´íŠ¸ (COMPLETED)
            logger.info(f"Finalizing job status to COMPLETED for job_id={job_id}")
            await _safe_update_status(JobStatus.COMPLETED.value, path=file_path, width=width, height=height, error_msg="")
            logger.info(f"âœ… Job {job_id} completed successfully: {file_path}")

            # 5. ë©”íƒ€ë°ì´í„° ìƒì„± íŠ¸ë¦¬ê±° (ë¹„ë™ê¸°)
            logger.info(f"Triggering metadata processing for job_id={job_id}")
            process_asset_metadata.delay(job_id)
            
        except Exception as e:
            logger.exception(f"âŒ Job {job_id} processing error: {e}")
            raise
    
    from asgiref.sync import async_to_sync
    import sqlalchemy as sa
    from app.infrastructure.database import sync_worker_engine
    
    # [Emergency Sync Update] ë¹„ë™ê¸° ë£¨í”„ê°€ ê¼¬ì˜€ì„ ë•Œë¥¼ ìœ„í•œ ë™ê¸°ì‹ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    def _sync_update_status(status_val: str, error_msg: Optional[str] = None):
        logger.warning(f"[HAWKEYE:SYNC] Emergency SYNC update initiated: {job_id} -> {status_val}")
        try:
            with sync_worker_engine.connect() as conn:
                assets_t = sa.table("assets", sa.column("job_id"), sa.column("status"), sa.column("error_message"))
                stmt = sa.update(assets_t).where(assets_t.c.job_id == job_id).values(
                    status=status_val
                )
                if error_msg is not None:
                    stmt = stmt.values(error_message=error_msg)
                result = conn.execute(stmt)
                conn.commit()
                logger.info(f"[HAWKEYE:SYNC] Emergency update SUCCESS: {result.rowcount} row(s) updated")
            
            # ğŸš€ [CRITICAL] ë™ê¸°ì‹ Redis ë°œí–‰ ì¶”ê°€
            from app.core.events import event_bus
            from datetime import datetime
            event_data = {
                "job_id": job_id,
                "status": status_val,
                "result_url": None,
                "error": error_msg,
                "updated_at": datetime.utcnow().isoformat() + "Z"
            }
            event_bus.publish_sync("asset_updates", event_data)
            logger.info(f"[HAWKEYE:SYNC] Emergency broadcast sent: {status_val}")
            
        except Exception as se:
            logger.error(f"[HAWKEYE:SYNC] CRITICAL: SYNC fallback failed: {se}")

    try:
        async_to_sync(_process)()
        logger.info(f"[HAWKEYE:WORKER] _process() async_to_sync finished normally for {job_id}")
    except Exception as exc:
        retry_count = self.request.retries
        max_retries = self.max_retries

        # ë””ë²„ê¹…ì„ ìœ„í•œ ì½”ë“œ
        exc_name = type(exc).__name__
        logger.error(f"[HAWKEYE:WORKER] Exception caught: {exc_name} for {job_id}. Current retry pool: {retry_count}/{max_retries}")
        # ë.

        if isinstance(exc, (ExternalServiceException, Exception)) and not isinstance(exc, ValidationException):
            if retry_count < max_retries:
                msg = f"ì¼ì‹œì  ì˜¤ë¥˜ë¡œ ì¬ì‹œë„ ì¤‘ ({retry_count + 1}/{max_retries})..."
                logger.info(f"[HAWKEYE:WORKER] Decision: RETRY. Attempt {retry_count + 1}")
                try:
                    async_to_sync(_safe_update_status)("PROCESSING", error_msg=msg)
                except Exception as update_err:
                    logger.warning(f"[HAWKEYE:WORKER] Async update failed before retry: {update_err}")
                    _sync_update_status("PROCESSING", error_msg=msg)
                
                raise self.retry(exc=exc, countdown=2 ** retry_count)
            else:
                error_detail = f"ìµœì¢… ì‹¤íŒ¨ ({max_retries}/{max_retries}): {str(exc)}"
                logger.error(f"[HAWKEYE:WORKER] Decision: PERMANENT FAILURE for {job_id}. Force-killing with SYNC update.")
                _sync_update_status("FAILED", error_msg=error_detail)
                raise exc
        elif isinstance(exc, ValidationException):
            logger.error(f"[HAWKEYE:WORKER] Decision: VALIDATION FAILURE. No retry.")
            _sync_update_status("FAILED", error_msg=str(exc))
            return
        else:
            _sync_update_status("FAILED", error_msg=f"Unexpected: {str(exc)}")
            raise exc

@celery_app.task(name="process_asset_metadata", bind=True, max_retries=3, soft_time_limit=60, time_limit=120)
def process_asset_metadata(self, job_id: str):
    """
    ì—ì…‹ì˜ ë©”íƒ€ë°ì´í„°(Search Document, Embedding)ë¥¼ ìƒì„±í•˜ëŠ” í›„ì²˜ë¦¬ ì‘ì—…
    """
    from app.infrastructure.database import worker_session_maker
    from app.infrastructure.repositories import SQLAlchemyAssetRepository
    from app.infrastructure.vertex_ai import get_ai_client
    from app.services.indexer import IndexerService
    from asgiref.sync import async_to_sync

    async def _process_metadata():
        logger.info(f"ğŸ” Starting metadata processing for job_id={job_id}")
        async with worker_session_maker() as session:
            async with session.begin():
                repo = SQLAlchemyAssetRepository(session)
                ai_client = get_ai_client()
                indexer = IndexerService(repo, ai_client)

                # job_idë¡œ asset ì¡°íšŒ
                asset = await repo.get_by_job_id(job_id)
                if not asset:
                    logger.error(f"âŒ Asset not found for metadata processing: {job_id}")
                    return

                await indexer.index_asset(asset.id)
                logger.info(f"âœ¨ Metadata processing completed for job_id={job_id}")

    try:
        # async_to_syncë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° í•¨ìˆ˜(_process_metadata)ë¥¼ ë™ê¸°ì‹ Celery ì›Œì»¤ì—ì„œ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        # ë‚´ë¶€ì ìœ¼ë¡œ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰ ì™„ë£Œ í›„ ì •ë¦¬í•©ë‹ˆë‹¤.
        async_to_sync(_process_metadata)()
    except Exception as e:
        logger.exception(f"âŒ Metadata processing failed for job_id={job_id}: {e}")
        # ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ íƒœìŠ¤í¬ ì‹¤íŒ¨ë¡œë§Œ ê¸°ë¡í•˜ê³  ì¬ì‹œë„
        retry_count = self.request.retries
        if retry_count < self.max_retries:
             raise self.retry(exc=e, countdown=2 ** retry_count)
        else:
            logger.error(f"âŒ Metadata processing permanently failed for {job_id}")
