import os
import time
import logging
from typing import List, Union
import numpy as np
from threading import Lock
from pathlib import Path

# optimum & onnxruntimeì€ ì„¤ì¹˜ ì—¬ë¶€ê°€ í™•ì¸ë˜ì—ˆìœ¼ë¯€ë¡œ ì§ì ‘ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from optimum.onnxruntime import ORTModelForFeatureExtraction
from optimum.exporters.onnx import main_export
from transformers import AutoTokenizer

# ë¡œê¹… ì„¤ì • (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ëŠ” ìˆ¨ê¸°ê³  ì•± ë¡œê·¸ë§Œ í‘œì‹œ)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("app.ml.model")
logger.setLevel(logging.INFO)

class EmbeddingModel:
    _instance = None
    _lock = Lock()
    
    MODEL_ID = "nlpai-lab/KURE-v1"
    # Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ ê²½ë¡œì— ë§ì¶° ì„¤ì •
    CACHE_DIR = "/app/storage/model_cache"

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(EmbeddingModel, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ (Target: {self.MODEL_ID})")
        self.model = None
        self.tokenizer = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.CACHE_DIR, exist_ok=True)
        
        self._load_model()
        self._initialized = True

    def _load_model(self):
        try:
            start_time = time.time()
            logger.info("ğŸ“¦ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_ID)
            
            # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
            model_path = Path(self.CACHE_DIR) / "model.onnx"
            
            if not model_path.exists():
                logger.info(f"ğŸ“¦ ONNX ëª¨ë¸ ë³€í™˜ ë° ë‚´ë³´ë‚´ê¸° ì‹œì‘ (ê²½ë¡œ: {self.CACHE_DIR})...")
                # optimum.exporters.onnx.main_exportë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ë³€í™˜
                # export=True ì˜µì…˜ì˜ ìˆ¨ê²¨ì§„ ì„ì‹œ íŒŒì¼ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•¨
                main_export(
                    self.MODEL_ID,
                    output=Path(self.CACHE_DIR),
                    task="feature-extraction",
                    opset=17, # ì•ˆì •ì ì¸ opset ë²„ì „ ì‚¬ìš©
                    do_validation=False
                )
                logger.info("ğŸ“¦ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ.")
            else:
                logger.info("ğŸ“¦ ê¸°ì¡´ ìºì‹œëœ ONNX ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

            # ë³€í™˜ëœ ëª¨ë¸ ë¡œë“œ (export=False)
            self.model = ORTModelForFeatureExtraction.from_pretrained(
                self.CACHE_DIR,
                file_name="model.onnx",
                provider="CPUExecutionProvider" 
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

    def encode(self, texts: Union[str, List[str]]) -> np.ndarray:
        """
        ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ë°˜í™˜ê°’: (N, 1024) í¬ê¸°ì˜ numpy ë°°ì—´
        """
        if isinstance(texts, str):
            texts = [texts]
            
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í† í¬ë‚˜ì´ì§• (PyTorch Tensor ë°˜í™˜)
        inputs = self.tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=512, 
            return_tensors="pt"
        )
        
        # ONNX Runtime ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•´ ì…ë ¥ ë³€í™˜ (Tensor -> Numpy)
        ort_inputs = {k: v.cpu().numpy() for k, v in inputs.items()}
        
        # ì¶”ë¡  (Bypass optimum forward to avoid KeyError)
        # self.model.model ì€ onnxruntime.InferenceSession ê°ì²´ì…ë‹ˆë‹¤.
        ort_outputs = self.model.model.run(None, ort_inputs)
        
        # ì¶œë ¥ ì´ë¦„ í™•ì¸ ë° ë°ì´í„° ì¶”ì¶œ
        output_names = [output.name for output in self.model.model.get_outputs()]
        
        embeddings = None
        
        # 1. sentence_embeddingì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (ì´ë¯¸ Pooling ë¨)
        if "sentence_embedding" in output_names:
            idx = output_names.index("sentence_embedding")
            embeddings = ort_outputs[idx]
        
        # 2. ì—†ìœ¼ë©´ token_embeddings(ë˜ëŠ” last_hidden_state)ë¥¼ ì°¾ì•„ Mean Pooling ìˆ˜í–‰
        elif "token_embeddings" in output_names or "last_hidden_state" in output_names:
            name = "token_embeddings" if "token_embeddings" in output_names else "last_hidden_state"
            idx = output_names.index(name)
            token_embeddings = ort_outputs[idx] # [batch, seq_len, hidden]
            
            # Numpyë¡œ Mean Pooling êµ¬í˜„
            attention_mask = ort_inputs['attention_mask'] # [batch, seq_len]
            
            # ì°¨ì› í™•ì¥
            input_mask_expanded = np.expand_dims(attention_mask, axis=-1)
            token_embeddings = token_embeddings * input_mask_expanded
            
            sum_embeddings = np.sum(token_embeddings, axis=1)
            sum_mask = np.sum(input_mask_expanded, axis=1)
            sum_mask = np.maximum(sum_mask, 1e-9) # 0 ë‚˜ëˆ„ê¸° ë°©ì§€
            
            embeddings = sum_embeddings / sum_mask
            
        else:
            raise KeyError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¶œë ¥ í˜•ì‹ì…ë‹ˆë‹¤: {output_names}")
        
        # ì„ë² ë”© ì •ê·œí™” (L2 Norm)
        norm = np.linalg.norm(embeddings, axis=1, keepdims=True)
        embeddings = embeddings / norm
        
        return embeddings

    def warmup(self):
        """
        ONNX ì„¸ì…˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ë”ë¯¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸ”¥ ëª¨ë¸ ì›œì—…(Warm-up) ì‹œì‘...")
        try:
            dummy_text = "Warm up the model."
            start = time.time()
            vectors = self.encode(dummy_text)
            elapsed = (time.time() - start) * 1000
            
            dim = vectors.shape[1]
            logger.info(f"ğŸ”¥ ì›œì—… ì™„ë£Œ! (ì°¨ì›: {dim}, ì†Œìš”ì‹œê°„: {elapsed:.2f}ms)")
            
            if dim != 1024:
                logger.warning(f"âš ï¸ ì˜ˆìƒ ì°¨ì›(1024)ê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {dim}")
                
        except Exception as e:
            logger.error(f"âŒ ì›œì—… ì‹¤íŒ¨: {e}")
            # ì—¬ê¸°ì„œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤.

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
embedding_model = EmbeddingModel()
